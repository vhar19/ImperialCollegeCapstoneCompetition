{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct, ConstantKernel as C\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide and Ideas for Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin this guide by downloading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS \n",
    "# This section defines the variables to run the rest of the document \n",
    "file = 1  # Identifies the function number to execute (1,2,3...8) \n",
    "resultsFile = \"data/results09.csv\"   # The latest results file provided by Carlton on a weekly basis \n",
    "kernel_name = \"polynomial\"   # Options are default, linear and polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputColumnName = \"f\" + str(file)\n",
    "outputColumnName = \"f\" + str(file) + \"_output\"\n",
    "df = pd.read_csv(resultsFile)\n",
    "input = df[inputColumnName]\n",
    "output = df[outputColumnName]\n",
    "print(\"Input=\", input)\n",
    "print(\"Output=\", output)\n",
    "df.head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD INITIAL DATA PROVIDED AT THE BEGINNING \n",
    "def load_data(dim):\n",
    "    X = np.load('data/initial_data/function_' + str(dim) +'/initial_inputs.npy')\n",
    "    y = np.load('data/initial_data/function_' + str(dim) +'/initial_outputs.npy')\n",
    "    return X,y\n",
    "\n",
    "# LOAD AND APPEND THE RESULTS PUBLISHED BY CARLTON \n",
    "X, y = load_data(file)\n",
    "dimension = np.shape(X)[1] \n",
    "\n",
    "for item in input:\n",
    "     item_with_comma = \"[\" + item.replace(\" 0.\",\", 0.\")  + \"]\"  \n",
    "     row = np.array(literal_eval(item_with_comma))\n",
    "     X = np.append(X, np.array(row), axis=0)\n",
    "\n",
    "for item in output:\n",
    "    y = np.append(y, float(item))\n",
    "\n",
    "print(\"X(shape)=\", X.shape, \"y(shape)=\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 2 functions can be shown as X,Y coordinates and reveal what is going on. On higher dimensions it loses meaning so we don't plot it \n",
    "if (dimension == 1 or dimension == 2):\n",
    "    x1 =X[:,0]\n",
    "    x2 =X[:,1] \n",
    "    plt.scatter(x1, x2, c=y)\n",
    "    for (i, j, text) in zip(x1, x2, y):\n",
    "        plt.annotate(format(text, \".2e\"),  # This is the text to use for the annotation\n",
    "                 (i, j),  # This is the point to label\n",
    "                 textcoords=\"offset points\",  # how to position the text\n",
    "                 xytext=(0,10),  # distance from text to points (x,y)\n",
    "                 ha='right')  # horizontal alignment can be left, right or center\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second alternative would be to use Bayesian Optimization and consider an Upper Confidence Bound acquisition function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add here hyper-parameters\n",
    "if (kernel_name == \"linear\"):\n",
    "    kernel = C(1.0, constant_value_bounds=\"fixed\") * DotProduct(sigma_0=1.0)\n",
    "else:\n",
    "    kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel=kernel)\n",
    "gpr.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maxiumum(X, y):\n",
    "    maximum_of_X = []\n",
    "    maximum_so_far = -1e99\n",
    "    for index in range(len(y)):\n",
    "        if (y[index] > maximum_so_far):\n",
    "            maximum_so_far = y[index]\n",
    "            maximum_of_X = []\n",
    "            for i in range(dimension):\n",
    "                maximum_of_X.append(X[index, i])\n",
    "    return np.array(maximum_of_X), maximum_so_far\n",
    "    \n",
    "discovery_radius = 0.2   # on each datapoint use 20% discovery radius\n",
    "\n",
    "Xmax, ymax = find_maxiumum(X, y)  #Returns the row with maximim y and all X values \n",
    "XmaxLower = Xmax - (Xmax * discovery_radius)\n",
    "XmaxUpper = Xmax + (Xmax * discovery_radius)\n",
    "print(\"maximum X=\", Xmax, \", maximum y=\", ymax)\n",
    "\n",
    "# Adjust sizes as we go up in dimensions to avoid waiting for hours on higher dimensions \n",
    "# ********* For the purpose of unblocking any memory issue please decrease these values to adjust to your computer specs ******************\n",
    "sizes = [ [2, 100],\n",
    "          [3,  80],\n",
    "          [4,  60],\n",
    "          [5,  30],\n",
    "          [6,  15],\n",
    "          [7,  10],\n",
    "          [8,  8]]\n",
    "\n",
    "size = sizes[dimension - 2][1]  #Find the corrects size for this dimension \n",
    "\n",
    "data = []\n",
    "for item in range(dimension):\n",
    "    #  Upper lower min and max explanation: \n",
    "    #  This iteration (each X value) creates a number of points of size \"size\" in a radius of +/-20% and ensures this radius is between 0 and 1, e.g.  \n",
    "    # - X values [0.5, 0, 0.99] \n",
    "    # - apply 20% discovery radius => [(0.4 to 0.6), (0 to 0), (0.792 to 1.188)] \n",
    "    # - apply 0 to 1 filter => [ linspace(0.4 to 0.6), linspace(0 to 0.0001), linspace(9.792 to 9.9999)]\n",
    "    # - result = [ [0.48777254 ... 0.59072617], [0.000002, ... 0.000099], [9.7921111, ...9.998888] ]\n",
    "    if (XmaxUpper[item] == 0):\n",
    "        XmaxUpper[item] = 0.0001\n",
    "    if (XmaxLower[item] == 1):\n",
    "        XmaxLower[item] = 0.9999\n",
    "    data.append(np.linspace(max(XmaxLower[item], 0 + 1e-10), min(XmaxUpper[item], 1 - 1e-10), size))\n",
    "\n",
    "x_grid_points = np.array(data)\n",
    "\n",
    "print(\"data-points=\", x_grid_points.shape)\n",
    "for dim in range(dimension):\n",
    "    print(\"data-points[\", dim + 1, \"]=\", x_grid_points[dim][:5], \"...\", x_grid_points[dim][-1] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grid search = cartesian product of the array, e.g. data of shape (2,100) => (100 x 100) = (10000, 2)\n",
    "X_grid = np.fromiter(it.chain(*it.product(*x_grid_points)), dtype=float).reshape(-1,dimension)\n",
    "print(\"X_grid=\", X_grid.shape, \" = \", X_grid[0:5,:], \"...\", X_grid[-1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = gpr.predict(X_grid, return_std = True)\n",
    "\n",
    "def calculate_ucb(beta):   \n",
    "    ucb = mean + beta * std\n",
    "    print(\"UCB=\", ucb.shape, \", value=\", ucb[:5])\n",
    "    print(\"mean=\", mean.shape, \", value=\", mean[:5])\n",
    "    idx_max = np.argmax(ucb)   # take the index of the highest confidence result \n",
    "    next_query = X_grid[idx_max]    # next_query is the point with highest confidence in this grid of search \n",
    "    best_numbers = []\n",
    "    for item in next_query:\n",
    "        best_numbers.append(min(max(item, 0.000001), 0.999999))   # ensure the numbers are between 0 and 1 \n",
    "    print(best_numbers)\n",
    "    return best_numbers\n",
    "\n",
    "def print_results(label, best_numbers):\n",
    "    #Formats the values in a way it can be copied and pasted into the weekly submission form \n",
    "    print(label)\n",
    "    print(np.array2string(np.array(best_numbers), precision=6, separator='-', floatmode='fixed', formatter={'float': '{:0.6f}'.format}))\n",
    "\n",
    "print(\"balanced\")\n",
    "balanced = calculate_ucb(1.96)  #keep it balanced\n",
    "\n",
    "print(\"\\n\\nexploitative\")\n",
    "exploitative = calculate_ucb(1.0)   #lets exploit target area\n",
    "\n",
    "print(\"\\n\\nexploratory\")\n",
    "exploratory = calculate_ucb(2.5)   #lets explore more \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_results(\"exploitative\", exploitative)\n",
    "\n",
    "print_results(\"\\nbalanced\", balanced)\n",
    "\n",
    "print_results(\"\\nexploratory\", exploratory)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d37abda7630e259e5026a5079657683a09f6e3d11473720762ebe7250c494840"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
